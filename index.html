<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yaqi Duan


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="shortcut icon" type="image/png" href="favicon.png">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-209081511-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-209081511-1');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Yaqi Duan
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/CV-YaqiDuan.pdf">
                CV
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                publications
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/publications_year/">publications by year</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="https://scholar.google.com/citations?user=T99vQCsAAAAJ">Google Scholar</a>
              
              
              </div>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/t-shirts/">
                ~misc.
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Yaqi Duan
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          
        </div>
      
    </div>
    

    <div class="clearfix">
      <p>Welcome to my homepage!
I am an Assistant Professor in the <a href="https://www.stern.nyu.edu/experience-stern/about/departments-centers-initiatives/academic-departments/ioms-dept">Department of Technology, Operations, and Statistics</a> at Stern School of Business at New York University. 
My primary research interests sit at the intersection of statistical machine learning and operations, centered on data-driven sequential decision making. Recently, I have been studying reinforcement learning for post-training large language models, exploring how data collection, exploration, and optimization dynamics shape learning.</p>

<p>I graduated with a Ph.D. degree from the <a href="https://orfe.princeton.edu/" target="\_blank">Department of Operations Research and Financial Engineering</a> at Princeton University in 2022. From 2022 to 2023, I was a postdoctoral researcher at the <a href="https://lids.mit.edu/" target="\_blank">Laboratory for Information &amp; Decision Systems</a> at Massachusetts Institute of Technology, working with Professor <a href="https://computing.mit.edu/martin-wainwright/" target="\_blank">Martin J. Wainwright</a>. Prior to my doctoral studies, I received a B.S. in Mathematics from Peking University.</p>

<p>If you are interested in working together, please feel free to reach out!</p>

<p>üìß <em> yaqi.duan </em> [At] stern [Dot] nyu [Dot] edu </p>
<p>üì¨ KMC 8-54, 44 West 4th Street, New York, NY 10012 </p>

    </div>

    
      <div class="news">
  <h1> </h1>
  <h3>News</h3>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jan 2026</th>
          <td>
            
              Paper <a href="https://duanyq22.github.io/publications_year/" target="\_blank">Stability through curvature: A framework for fast convergence in reinforcement learning</a> has received a Minor Revision decision from Operations Research after the first round of review.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 2025</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2601.00121" target="\_blank">Ask, clarify, optimize: Human-LLM agent collaboration for smarter inventory control</a> posted on arXiv. <br />
We position LLMs as intelligent collaborators rather than replacements for operations research, enabling effective human‚ÄìLLM coordination for inventory control through structured agentic design.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 2025</th>
          <td>
            
              Talk at the <a href="https://meetings.informs.org/wordpress/annual/" target="\_blank">2025 INFORMS Annual Meeting</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 2025</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2510.08696" target="\_blank">Don‚Äôt waste mistakes: Leveraging negative RL-groups via confidence reweighting
</a> posted on arXiv. <br />
We show that negative groups in RLVR can be exploited without extra supervision by reinterpreting MLE as a policy gradient with confidence-weighted penalties, leading to the LENS algorithm.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 2025</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2510.08539" target="\_blank">On the optimization dynamics of RLVR: Gradient gap and step size thresholds
</a> posted on arXiv. <br />
We develop a theoretical understanding of RLVR optimization dynamics by introducing the Gradient Gap, which characterizes convergence directions and sharp step-size thresholds underlying stability and failure.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 2025</th>
          <td>
            
              Talk at the <a href="https://ww2.amstat.org/meetings/jsm/2025/" target="\_blank">2025 Joint Statistical Meetings</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 2025</th>
          <td>
            
              I am honored to receive the <a href="https://www.lse.ac.uk/news/latest-news-from-lse/lse-and-nyu-announce-first-awards-for-joint-research-seed-funding" target="\_blank">LSE‚ÄìNYU Research Seed Fund</a>. I am grateful to collaborate with Professor <a href="https://stats.lse.ac.uk/q.yao/" target="\_blank">Qiwei Yao</a> from LSE and thankful for this opportunity. Look forward to our work together!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 2025</th>
          <td>
            
              Excited to welcome <a href="https://joesuk.github.io/" target="\_blank">Joe Suk</a> as a Postdoctoral Researcher and look forward to our collaboration!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 2025</th>
          <td>
            
              Talk at the <a href="https://informs-aps.isye.gatech.edu/" target="\_blank">2025 INFORMS Applied Probability Conference</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 2025</th>
          <td>
            
              Paper <a href="https://openreview.net/forum?id=Qap9pHIkI8" target="\_blank">PILAF: Optimal human preference sampling for reward modeling</a> accepted by ICML 2025.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 2025</th>
          <td>
            
              Talk at the <a href="https://stat.columbia.edu/optimization-and-statistical-learning-workshop/" target="\_blank">Optimization and Statistical Learning Workshop</a>, Columbia University.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 2025</th>
          <td>
            
              Talk at the <a href="https://cims.nyu.edu/ai/seminars/cilvr-seminar-series/61/" target="\_blank">CILVR Seminar</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 2025</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2502.04270" target="\_blank">PILAF: Optimal human preference sampling for reward modeling</a> posted on arXiv! <br />
We introduce PILAF, a simple yet effective  algorithm for data collection in RLHF, showing its efficiency both theoretically and empirically.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 2024</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2412.19252" target="\_blank">Localized exploration in contextual dynamic pricing achieves dimension-free regret</a> posted on arXiv.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 2024</th>
          <td>
            
              Talk at the <a href="https://sites.google.com/view/rltheoryseminars/home" target="\_blank">RL Theory Seminar</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 2024</th>
          <td>
            
              Talk at the <a href="https://statistics.rutgers.edu/news-events/seminars" target="\_blank">Department of Statistics</a>, Rutgers University.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 2024</th>
          <td>
            
              Paper <a href="https://openreview.net/forum?id=CbHz30KeA4&amp;noteId=JoGxiOkQem" target="\_blank">Taming ‚Äúdata-hungry‚Äù reinforcement learning? Stability in continuous state-action spaces</a> accepted by NeurIPS 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 2024</th>
          <td>
            
              Talk at the <a href="https://orfe.princeton.edu/events/wilks-seminar" target="\_blank">S. S. Wilks Memorial Seminar in Statistics</a>, Princeton University.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 2024</th>
          <td>
            
              I am honored to receive my first <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2413812&amp;HistoricalAwards=false" target="\_blank">NSF grant</a>. Grateful for this opportunity!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 2024</th>
          <td>
            
              Paper <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-52/issue-5/Optimal-policy-evaluation-using-kernel-based-temporal-difference-methods/10.1214/24-AOS2399.short" target="\_blank">Optimal policy evaluation using kernel-based temporal difference methods</a> accepted by the Annals of Statistics.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 2024</th>
          <td>
            
              Talk at the <a href="https://cims.nyu.edu/dynamic/calendars/seminars/math-and-data-seminar/spring-2024/" target="\_blank">Math &amp; Data (MaD) Seminar</a>, New York University.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 2024</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2401.05233" target="\_blank">Taming ‚Äúdata-hungry‚Äù reinforcement learning? Stability in continuous state-action spaces</a> posted on arXiv.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 2023</th>
          <td>
            
              Paper <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-5/Adaptive-and-robust-multi-task-learning/10.1214/23-AOS2319.short" target="\_blank">Adaptive and robust multi-task learning</a> accepted by the Annals of Statistics.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 2023</th>
          <td>
            
              I‚Äôve joined NYU Stern School of Business as an Assistant Professor in the <a href="https://www.stern.nyu.edu/experience-stern/about/departments-centers-initiatives/academic-departments/ioms-dept">Department of Technology, Operations, and Statistics</a>. Thrilled to embark on this new academic journey!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 2022</th>
          <td>
            
              New paper <a href="https://arxiv.org/abs/2211.03899" target="\_blank">Policy evaluation from a single path: Multi-step methods, mixing and mis-specification</a> posted on arXiv.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 2022</th>
          <td>
            
              I am honored to receive the <a href="https://imstat.org/ims-awards/ims-lawrence-d-brown-ph-d-student-award/" target="\_blank">2023 IMS Lawrence D. Brown Ph.D. Student Award</a>.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h3>Selected publications</h3>
  <h2> </h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="suk2025optimization" class="col-sm-8">
    
      <div class="title">On the optimization dynamics of RLVR: Gradient gap and step size thresholds</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Suk, Joe,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Duan, Yaqi</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv Preprint</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2510.08539" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has shown significant empirical success. However, a principled understanding of why it works has been lacking. This paper builds a theoretical foundation for RLVR by analyzing its training process at both the full-response (trajectory) and token levels. Central to our analysis is a quantity called the Gradient Gap, which formalizes the direction of improvement from low-reward to high-reward regions of the response space. We prove that convergence critically depends on aligning the update direction with this Gradient Gap. Moreover, we derive a sharp step-size threshold based on the magnitude of the Gradient Gap: below it, learning converges, whereas above it, performance collapses. Our theory further predicts how the critical step size must scale with response length and the success rate, thereby explaining why practical heuristics such as length normalization improve stability and showing that, with a fixed learning rate, the success rate can stagnate strictly below 100%. We validate these predictions through controlled bandit simulations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="feng2025pilaf" class="col-sm-8">
    
      <div class="title">PILAF: Optimal human preference sampling for reward modeling
	</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Feng, Yunzhen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kwiatkowski, Ariel‚Ä†,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Kunhao‚Ä†,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kempe, Julia*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Duan, Yaqi*</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML)</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2502.04270" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="duan2025stability" class="col-sm-8">
    
      <div class="title">Stability through curvature: A framework for fast convergence in reinforcement learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Duan, Yaqi</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wainwright, Martin J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Minor revision (first round), Operations Research</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect system dynamics via the Bellman operator and induced occupation measure. We show that these stability properties can be ensured by sufficient curvature in the function class used to represent state-action value functions. Curvature is a static property, making it easier to verify than the dynamic stability conditions that it implies. Using linear function represetation, we illustrate the connection between curvature and stability with concrete formulations. Overall, the key insight is that function classes with sufficient curvature guarantee dynamic system stability, leading to fast convergence rates in RL.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="chai2024localized" class="col-sm-8">
    
      <div class="title">Localized exploration in contextual dynamic pricing achieves dimension-free regret</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Chai, Jinhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Duan, Yaqi</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fan, Jianqing,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wang, Kaizheng (Œ±-Œ≤)
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv Preprint</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2412.19252" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the problem of contextual dynamic pricing with a linear demand model. We propose a novel localized exploration-then-commit (LetC) algorithm which starts with a pure exploration stage, followed by a refinement stage that explores near the learned optimal pricing policy, and finally enters a pure exploitation stage. The algorithm is shown to achieve a minimax optimal, dimension-free regret bound when the time horizon exceeds a polynomial of the covariate dimension. Furthermore, we provide a general theoretical framework that encompasses the entire time spectrum, demonstrating how to balance exploration and exploitation when the horizon is limited. The analysis is powered by a novel critical inequality that depicts the exploration-exploitation trade-off in dynamic pricing, mirroring its existing counterpart for the bias-variance trade-off in regularized regression. Our theoretical results are validated by extensive experiments on synthetic and real-world data.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="duan2021optimal" class="col-sm-8">
    
      <div class="title">Optimal policy evaluation using kernel-based temporal difference methods</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Duan, Yaqi</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Mengdi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wainwright, Martin J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The Annals of Statistics</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2109.12002" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study non-parametric methods for estimating the value function of an infinite-horizon discounted Markov reward process (MRP). We establish non-asymptotic bounds on the statistical error of a kernel-based least-squares temporal difference (LSTD) estimate, which can be understood either as a non-parametric instrumental variables method, or as a projected approximation to the Bellman fixed point equation. Our analysis imposes no assumptions on the transition operator of the Markov chain, but rather only conditions on the reward function and population-level kernel LSTD solutions. Using empirical process theory and concentration inequalities, we establish a non-asymptotic upper bound on the error with explicit dependence on the effective horizon H = 1/(1-Œ≥) of the Markov reward process, the eigenvalues of the associated kernel operator, as well as the instance-dependent variance of the Bellman residual error. In addition, we prove minimax lower bounds over sub-classes of MRPs, which shows that our rate is optimal in terms of the sample size n and the effective horizon H. Whereas existing worst-case theory predicts cubic scaling (H^3) in the effective horizon, our theory reveals that there is in fact a much wider range of scalings, depending on the kernel, the stationary distribution, and the variance of the Bellman residual error. Notably, it is only parametric and near-parametric problems that can ever achieve the worst-case cubic scaling.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="duan2022adaptive" class="col-sm-8">
    
      <div class="title">Adaptive and robust multi-task learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Duan, Yaqi</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wang, Kaizheng (Œ±-Œ≤)
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The Annals of Statistics</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2202.05250" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the multi-task learning problem that aims to simultaneously analyze multiple datasets collected from different sources and learn one model for each of them. We propose a family of adaptive methods to automatically utilize possible similarities among those tasks while carefully handling their differences. We derive optimal statistical guarantees for the methods and prove their robustness against outlier tasks. Numerical experiments on synthetic and real datasets demonstrate the efficacies of our new methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="duan2020minimax" class="col-sm-8">
    
      <div class="title">Minimax-optimal off-policy evaluation with linear function approximation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Duan, Yaqi</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wang, Mengdi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2002.09516" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies the statistical theory of batch data reinforcement learning with function approximation. Consider the off-policy evaluation problem, which is to estimate the cumulative value of a new target policy from logged history generated by unknown behavioral policies. We study a regression-based fitted Q iteration method, and show that it is equivalent to a model-based method that estimates a conditional mean embedding of the transition operator. We prove that this method is information-theoretically optimal and has nearly minimal estimation error. In particular, by leveraging contraction property of Markov processes and martingale concentration, we establish a finite-sample instance-dependent error upper bound and a nearly-matching minimax lower bound. The policy evaluation error depends sharply on a restricted œá^2-divergence over the function class between the long-term distribution of the target policy and the distribution of past data. This restricted œá^2-divergence is both instance-dependent and function-class-dependent. It characterizes the statistical limit of off-policy evaluation. Further, we provide an easily computable confidence bound for the policy evaluator, which may be useful for optimistic planning and safe policy improvement.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Yaqi  Duan.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
